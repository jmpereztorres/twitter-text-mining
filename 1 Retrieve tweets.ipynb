{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trabajo Text mining en Twitter\n",
    "\n",
    "El trabajo básico no presencial de \"Text Mining en Twitter\" consistirá en desarrollar un pequeño sistema de Sentiment Analysis siguiendo los siguientes pasos:\n",
    " \n",
    "0) obtener una cuenta de desarrollo en Twitter (apps.twitter.com), registrar una aplicación y las claves correspondiente.\n",
    "\n",
    "1) descargar los tweets de entrenamiento a partir del id (TASS_training_polarity.txt).\n",
    "\n",
    "2) utilizar algún método de machine learning para construir un clasificador.\n",
    "\n",
    "3) descargar los tweets de test a partir del id (TASS_test_ids.txt).\n",
    "\n",
    "4) etiquetar los tweets de test utilizando el modelo aprendido en 2).\n",
    "\n",
    "5) subir a la actividad de PoliformaT un fichero de texto con el test etiquetado (identificador \\t polaridad) y un fichero \".zip\" con todo el código desarrollado.  \n",
    "\n",
    "6) para mejorar los modelos se puede utilizar un diccionario de polaridad (ElhPolar_esV1.lex )\n",
    "\n",
    "El formato del fichero de test debe ser igual que el de train (TASS_training_polarity.txt) es decir:\n",
    "\n",
    "identificador1 (tabulador) polaridad1\n",
    "identificador2 (tabulador) polaridad2\n",
    "identificador3 (tabulador) polaridad3\n",
    "\n",
    "Se debe subir todo el código y el fichero del test etiquetado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import twitter credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tweepy.api.API object at 0x00000221083E9DD8>\n"
     ]
    }
   ],
   "source": [
    "import tweepy\n",
    "import json\n",
    "\n",
    "from twitter_secret import *\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "API = tweepy.API(auth)\n",
    "\n",
    "print (API)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read text TASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "text_file = open('TASS_training_polarity.txt', 'r')\n",
    "#text_file = open('TASS_test_ids.txt', 'r')\n",
    "\n",
    "list = {}\n",
    "idList = []\n",
    "\n",
    "for row in csv.reader(text_file, delimiter='\\t'):\n",
    "    id = row[0]\n",
    "    if(len(row) > 1):\n",
    "        score = row[1]\n",
    "    else:\n",
    "        score = None\n",
    "    \n",
    "    json = {id: {'score': score}} if score else {id: {}}\n",
    "        \n",
    "    idList.append(id)\n",
    "    list.update(json)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read tweets\n",
    "\n",
    "Twitter API allows to retrieve tweets 100 by 100, so page the ids in order to fit their contraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "numPages = int(math.ceil(len(idList)/100))\n",
    "\n",
    "for i in range(0, numPages):\n",
    "    tweetPage = API.statuses_lookup(idList[i*100:i*100+99])\n",
    "    for idx, tweet in enumerate(tweetPage):\n",
    "        if(list.get(tweet.id_str).get('score')):\n",
    "            json = { tweet.id_str: {'score': list.get(tweet.id_str).get('score'), 'text': tweet.text, 'user': tweet.user.screen_name }  }\n",
    "        else:\n",
    "            json = { tweet.id_str: {'text': tweet.text, 'user': tweet.user.screen_name } }\n",
    "        \n",
    "        list.update(json)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform to csv rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descargados correctamente: 6844/7219\n"
     ]
    }
   ],
   "source": [
    "dataRows = []\n",
    "for key, value in list.items():\n",
    "    row = []\n",
    "    if('text' in value):\n",
    "        row.append(key)\n",
    "        row.append(value['text'].replace('\\n', ' '))\n",
    "        row.append(value['score'])\n",
    "        row.append(value['user'])\n",
    "        dataRows.append(row)\n",
    "    \n",
    "# leemos el % de tweets que han sido descargados correctamente\n",
    "print('Descargados correctamente: {}/{}'.format(len(dataRows), len(list.items())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save to csv, not to load them all again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import csv\n",
    "\n",
    "def save_tweets(filename, tweets):\n",
    "    with open(filename, 'w', encoding='utf-8') as fh:\n",
    "        for tweet in tweets:\n",
    "            line = '\\t'.join(tweet)\n",
    "            fh.write(line + '\\n')\n",
    "    fh.close()\n",
    "\n",
    "save_tweets('tweets.csv', dataRows)\n",
    "\n",
    "print('Saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
